{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 latent methods for dimension reduction and topic modeling\n",
    "\n",
    "![](https://cdn.pixabay.com/photo/2015/11/07/11/17/golden-gate-bridge-1030999_960_720.jpg)\n",
    "Photo: https://pixabay.com/en/golden-gate-bridge-women-back-1030999/\n",
    "\n",
    "Before the state-of-the-art word embedding technique, Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) area good approaches to deal with NLP problems. Both LSA and LDA have same input which is Bag of words in matrix format. LSA focus on reducing matrix dimension while LDA solves topic modeling problems.\n",
    "\n",
    "I will not go through mathematical detail and as there is lot of great material for that. You may check it from reference. For the sake of keeping it easy to understand, I did not do pre-processing such as stopwords removal. It is critical part when you use LSA, LSI and LDA. After reading this article, you will know:\n",
    "- Latent Semantic Analysis (LSA)\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Take Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_raw = fetch_20newsgroups(subset='train')\n",
    "test_raw = fetch_20newsgroups(subset='test')\n",
    "\n",
    "x_train = train_raw.data\n",
    "y_train = train_raw.target\n",
    "\n",
    "x_test = test_raw.data\n",
    "y_test = test_raw.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Senmantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that words will occurs in similar pieces of text if they have similar meaning. People usually use Latent Semantic Indexing (LSI) as an alternative name in NLP field.\n",
    "\n",
    "First of all, we have m documents and n words as input. An m * n matrix can be constructed while column and row are document and word respectively. You can use count occurrence or TF-IDF score. However, TF-IDF is better than count occurrence in most of the time as high frequency do not account for better classification.\n",
    "\n",
    "![](https://1.bp.blogspot.com/-tnzPA6dDtTU/Vw6EWm_PjCI/AAAAAAABDwI/JatHtUJb4fsce9E-Ns5t02_nakFtGrsugCLcB/s1600/%25E8%259E%25A2%25E5%25B9%2595%25E5%25BF%25AB%25E7%2585%25A7%2B2016-04-14%2B%25E4%25B8%258A%25E5%258D%25881.39.07.png)\n",
    "Photo: http://mropengate.blogspot.com/2016/04/tf-idf-in-r-language.html\n",
    "\n",
    "The idea of TF-IDF is that high frequency may not able to provide much information gain. In another word, rare words contribute more weights to the model. Word importance will be increased if the number of occurrence within same document (i.e. training record). On the other hand, it will be decreased if it occurs in corpus (i.e. other training records). For detail, you may check this [blog](https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016).\n",
    "\n",
    "The challenge is that the matrix is very sparse (or high dimension) and noisy (or include lots of low frequency word). So truncated SVD is adopted to reduce dimension.\n",
    "\n",
    "![]()\n",
    "\n",
    "The idea of SVD is finding the most valuable information and using lower dimension t to represent same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF output shape: (11314, 130107)\n",
      "LSA output shape: (11314, 50)\n",
      "Sum of explained variance ratio: 8%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def build_lsa(x_train, x_test, dim=50):\n",
    "    tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    \n",
    "    transformed_x_train = tfidf_vec.fit_transform(x_train)\n",
    "    transformed_x_test = tfidf_vec.transform(x_test)\n",
    "    \n",
    "    print('TF-IDF output shape:', transformed_x_train.shape)\n",
    "    \n",
    "    x_train_svd = svd.fit_transform(transformed_x_train)\n",
    "    x_test_svd = svd.transform(transformed_x_test)\n",
    "    \n",
    "    print('LSA output shape:', x_train_svd.shape)\n",
    "    \n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance * 100)))\n",
    "    \n",
    "    return tfidf_vec, svd, x_train_svd, x_test_svd\n",
    "\n",
    "\n",
    "tfidf_vec, svd, x_train_lda, x_test_lda = build_lsa(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dimension reduces from 130k to 50 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6511 (+/- 0.0201)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "lr_model = LogisticRegression(solver='newton-cg',n_jobs=-1)\n",
    "lr_model.fit(x_train_svd, y_train)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "scores = cross_val_score(lr_model, x_test_svd, y_test, cv=cv, scoring='accuracy')\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is introduced by David Blei, Andrew Ng and Michael O. Jordan in 2003. It is unsupervised learning and topic model is the typical example. The assumption is that each document mix with various topics and every topic mix with various words.\n",
    "\n",
    "![]()\n",
    "\n",
    "Intuitively, you can image that we have two layer of aggregations. First layer is the distribution of categories. For example, we have finance news, weather news and political news. Second layer is distribution of words within the category. For instance, we can find \"sunny\" and \"cloud\" in weather news while \"money\" and \"stock\" exists in finance news. \n",
    "\n",
    "However,  \"a\", \"with\" and \"can\" do not contribute on topic modeling problem. Those words exist among documents and will have roughly same probability between categories. Therefore, stopwords removal is a critical step to achieve a better result.\n",
    "\n",
    "![]()\n",
    "\n",
    "For particular document d, we get the topic distribution which is θ. From this distribution(θ), topic t will be chosen and selecting corresponding word from ϕ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['the', 'for', 'and', 'to', 'edu']\n",
      "Topic 1:\n",
      "['c_', 'w7', 'hz', 'mv', 'ck']\n",
      "Topic 2:\n",
      "['space', 'nasa', 'cmu', 'science', 'edu']\n",
      "Topic 3:\n",
      "['the', 'to', 'of', 'for', 'and']\n",
      "Topic 4:\n",
      "['the', 'to', 'of', 'and', 'in']\n",
      "Topic 5:\n",
      "['the', 'of', 'and', 'in', 'were']\n",
      "Topic 6:\n",
      "['edu', 'team', 'he', 'game', '10']\n",
      "Topic 7:\n",
      "['ax', 'max', 'g9v', 'b8f', 'a86']\n",
      "Topic 8:\n",
      "['db', 'bike', 'ac', 'image', 'dod']\n",
      "Topic 9:\n",
      "['nec', 'mil', 'navy', 'sg', 'behanna']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def build_lda(x_train, num_of_topic=10):\n",
    "    vec = CountVectorizer()\n",
    "    transformed_x_train = vec.fit_transform(x_train)\n",
    "    feature_names = vec.get_feature_names()\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=num_of_topic, max_iter=5, \n",
    "        learning_method='online', random_state=0)\n",
    "    lda.fit(transformed_x_train)\n",
    "\n",
    "    return lda, vec, feature_names\n",
    "\n",
    "def display_word_distribution(model, feature_names, n_word):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        words = []\n",
    "        for i in topic.argsort()[:-n_word - 1:-1]:\n",
    "            words.append(feature_names[i])\n",
    "        print(words)\n",
    "\n",
    "lda_model, vec, feature_names = build_lda(x_train)\n",
    "display_word_distribution(\n",
    "    model=lda_model, feature_names=feature_names, \n",
    "    n_word=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Away\n",
    "- Both of them use __Bag-of-words as input matrix__\n",
    "- The challenge of SVD is that we are __hard to determine the optimal number of dimension__. In general, low dimension consume less resource but we may not able to distinguish opposite meaning words while high dimension overcome it but consuming more resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Me\n",
    "I am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence , especially in NLP and platform related. You can reach me from [Medium Blog](https://medium.com/@makcedward) or [Github](https://github.com/makcedward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- [1] SVD Tutorial: https://cs.fit.edu/~dmitra/SciComp/Resources/singular-value-decomposition-fast-track-tutorial.pdf\n",
    "- [2] CUHK LSI Tutorial: http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf\n",
    "- [3] Stanford LSI Tutorial: https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf\n",
    "- [4] LSA and LDA Explanation: https://cs.stanford.edu/~ppasupat/a9online/1140.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
